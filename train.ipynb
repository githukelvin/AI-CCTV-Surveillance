{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6K6O1mE7bE3",
        "outputId": "02bd1183-19ad-4951-f7af-ca02cb967c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNEqwGzTPgDW"
      },
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Path to your zip file\n",
        "# zip_path = '/content/drive/MyDrive/actionRecognition/crime16.zip'\n",
        "\n",
        "# # Create extraction directory (if it doesn't exist)\n",
        "# extract_path = '/content/drive/MyDrive/actionRecognition/crime16'\n",
        "# os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# # Unzip the file\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_path)\n",
        "\n",
        "# print(f\"Files extracted to: {extract_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfdq8bSO7s0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7bd358-17f6-4667-8493-6ac691a4f446"
      },
      "source": [
        "import torchvision\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import os\n",
        "from tqdm.autonotebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import cv2\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-a0647214ad17>:9: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFDscTBfPNt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa5b3fdb-6448-43c8-b0d2-64bb13a24bb8"
      },
      "source": [
        "!git clone https://github.com/sanchit2843/Videoclassification\n",
        "%cd Videoclassification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Videoclassification'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Total 51 (delta 0), reused 0 (delta 0), pack-reused 51 (from 1)\u001b[K\n",
            "Receiving objects: 100% (51/51), 100.12 KiB | 5.27 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n",
            "/content/Videoclassification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__TVncCdN1jN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e8d978-8772-48e5-85f7-715e93cb9159"
      },
      "source": [
        "#Label file:\n",
        "data_path = '/content/drive/MyDrive/actionRecognition/crime16'\n",
        "classes = os.listdir(data_path)\n",
        "print(classes)\n",
        "decoder = {}\n",
        "for i in range(len(classes)):\n",
        "    decoder[classes[i]] = i\n",
        "encoder = {}\n",
        "for i in range(len(classes)):\n",
        "    encoder[i] = classes[i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Robbery', 'Vandalism', 'Shoplifting', 'normal', 'Burglary', 'Stealing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiuCb0FPCn87"
      },
      "source": [
        "id = list()\n",
        "path = '/content/drive/MyDrive/actionRecognition/crime16'\n",
        "for i in os.listdir(path):\n",
        "  p1 = os.path.join(path,i)\n",
        "  for j in os.listdir(p1):\n",
        "    p2 = os.path.join(p1,j)\n",
        "    id.append((i,p2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5iUVP4p6VFn"
      },
      "source": [
        "class video_dataset(Dataset):\n",
        "    def __init__(self,frame_list,sequence_length = 16,transform = None):\n",
        "        self.frame_list = frame_list\n",
        "        self.transform = transform\n",
        "        self.sequence_length = sequence_length\n",
        "    def __len__(self):\n",
        "        return len(self.frame_list)\n",
        "    def __getitem__(self,idx):\n",
        "        label,path = self.frame_list[idx]\n",
        "        img = cv2.imread(path)\n",
        "        seq_img = list()\n",
        "        for i in range(16):\n",
        "          img1 = img[:,128*i:128*(i+1),:]\n",
        "          if(self.transform):\n",
        "            img1 = self.transform(img1)\n",
        "          seq_img.append(img1)\n",
        "        seq_image = torch.stack(seq_img)\n",
        "        seq_image = seq_image.reshape(3,16,im_size,im_size)\n",
        "        return seq_image,decoder[label]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msB4umEmPxj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9f3e10-328a-43ba-d3fa-3c86786e3054"
      },
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "# Parameters\n",
        "im_size = 128\n",
        "mean = [0.4889, 0.4887, 0.4891]\n",
        "std = [0.2074, 0.2074, 0.2074]\n",
        "validation_split = 0.2\n",
        "batch_size = 8\n",
        "\n",
        "# Define transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((im_size, im_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# For validation, we don't need augmentation transforms\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((im_size, im_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Create full dataset\n",
        "full_dataset = video_dataset(id, sequence_length=16, transform=train_transforms)\n",
        "\n",
        "# Calculate lengths for split\n",
        "total_size = len(full_dataset)\n",
        "val_size = int(validation_split * total_size)\n",
        "train_size = total_size - val_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(\n",
        "    full_dataset,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)  # for reproducibility\n",
        ")\n",
        "\n",
        "# Override transforms for validation dataset\n",
        "val_dataset.dataset.transform = val_transforms\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4,\n",
        "    shuffle=False  # No need to shuffle validation data\n",
        ")\n",
        "\n",
        "# Create dataloaders dictionary\n",
        "dataloaders = {\n",
        "    'train': train_loader,\n",
        "    'val': val_loader\n",
        "}\n",
        "\n",
        "print(f\"Training set size: {train_size}\")\n",
        "print(f\"Validation set size: {val_size}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 4000\n",
            "Validation set size: 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqtR9UDF72pX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "d415da4e-a8cc-4165-f3a9-9edf43520762"
      },
      "source": [
        "from model import resnet50\n",
        "model = resnet50(class_num=6).to('cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-bee83bf7e5e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqyU-0k67-N_"
      },
      "source": [
        "from clr import *\n",
        "device = 'cpu'\n",
        "cls_criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum = 0.9,weight_decay = 1e-4)\n",
        "num_epochs = 20\n",
        "onecyc = OneCycle(len(train_loader)*num_epochs,1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybCCnE-T8GzU"
      },
      "source": [
        "# os.makedirs('/content/drive/MyDrive/actionRecognition/weights_crime',exist_ok = True)\n",
        "# from torch.autograd import Variable\n",
        "# iteration = 0\n",
        "# acc_all = list()\n",
        "# loss_all = list()\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     print('')\n",
        "#     print(f\"--- Epoch {epoch} ---\")\n",
        "#     phase1 = dataloaders.keys()\n",
        "#     for phase in phase1:\n",
        "#         print('')\n",
        "#         print(f\"--- Phase {phase} ---\")\n",
        "#         epoch_metrics = {\"loss\": [], \"acc\": []}\n",
        "#         for batch_i, (X, y) in enumerate(dataloaders[phase]):\n",
        "#             #iteration = iteration+1\n",
        "#             image_sequences = Variable(X.to(device), requires_grad=True)\n",
        "#             labels = Variable(y.to(device), requires_grad=False)\n",
        "#             optimizer.zero_grad()\n",
        "#             #model.lstm.reset_hidden_state()\n",
        "#             predictions = model(image_sequences)\n",
        "#             loss = cls_criterion(predictions, labels)\n",
        "#             acc = 100 * (predictions.detach().argmax(1) == labels).cpu().numpy().mean()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             epoch_metrics[\"loss\"].append(loss.item())\n",
        "#             epoch_metrics[\"acc\"].append(acc)\n",
        "#             if(phase=='train'):\n",
        "#                 lr,mom = onecyc.calc()\n",
        "#                 update_lr(optimizer, lr)\n",
        "#                 update_mom(optimizer, mom)\n",
        "#             batches_done = epoch * len(dataloaders[phase]) + batch_i\n",
        "#             batches_left = num_epochs * len(dataloaders[phase]) - batches_done\n",
        "#             sys.stdout.write(\n",
        "#                     \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f), Acc: %.2f%% (%.2f%%)]\"\n",
        "#                     % (\n",
        "#                         epoch,\n",
        "#                         num_epochs,\n",
        "#                         batch_i,\n",
        "#                         len(dataloaders[phase]),\n",
        "#                         loss.item(),\n",
        "#                         np.mean(epoch_metrics[\"loss\"]),\n",
        "#                         acc,\n",
        "#                         np.mean(epoch_metrics[\"acc\"]),\n",
        "#                     )\n",
        "#                 )\n",
        "\n",
        "#                 # Empty cache\n",
        "#             if torch.cuda.is_available():\n",
        "#                 torch.cuda.empty_cache()\n",
        "\n",
        "#         print('')\n",
        "#         print('{} , acc: {}'.format(phase,np.mean(epoch_metrics[\"acc\"])))\n",
        "#         torch.save(model.state_dict(),'/content/drive/MyDrive/actionRecognition/weights_crime/c3d_{}.h5'.format(epoch))\n",
        "#         if(phase=='train'):\n",
        "#           acc_all.append(np.mean(epoch_metrics[\"acc\"]))\n",
        "#           loss_all.append(np.mean(epoch_metrics[\"loss\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH7XMOAjV7DH"
      },
      "source": [
        "def error_plot(loss):\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(loss)\n",
        "    plt.title(\"Training loss plot\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "def acc_plot(acc):\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(acc)\n",
        "    plt.title(\"Training accuracy plot\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "def train_and_evaluate(model, dataloaders, optimizer, num_epochs, device, onecyc):\n",
        "    os.makedirs('/content/drive/MyDrive/actionRecognition/weights_crime', exist_ok=True)\n",
        "\n",
        "    # Initialize scaler for mixed precision training\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # Track metrics\n",
        "    acc_all = []\n",
        "    loss_all = []\n",
        "    best_acc = 0.0\n",
        "    best_model_path = ''\n",
        "\n",
        "    # Loss criterion\n",
        "    cls_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\n--- Epoch {epoch}/{num_epochs-1} ---\")\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            epoch_metrics = {\"loss\": [], \"acc\": []}\n",
        "\n",
        "            # Enable gradient computation only during training\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                for batch_i, (X, y) in enumerate(dataloaders[phase]):\n",
        "                    # Move data to GPU\n",
        "                    image_sequences = X.to(device)\n",
        "                    labels = y.to(device)\n",
        "\n",
        "                    # Clear gradients\n",
        "                    optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
        "\n",
        "                    # Mixed precision training\n",
        "                    with autocast():\n",
        "                        predictions = model(image_sequences)\n",
        "                        loss = cls_criterion(predictions, labels)\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    acc = 100 * (predictions.detach().argmax(1) == labels).cpu().numpy().mean()\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        # Scale loss and backward pass\n",
        "                        scaler.scale(loss).backward()\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "\n",
        "                        # Update learning rate and momentum\n",
        "                        lr, mom = onecyc.calc()\n",
        "                        update_lr(optimizer, lr)\n",
        "                        update_mom(optimizer, mom)\n",
        "\n",
        "                    # Track metrics\n",
        "                    epoch_metrics[\"loss\"].append(loss.item())\n",
        "                    epoch_metrics[\"acc\"].append(acc)\n",
        "\n",
        "                    # Progress update\n",
        "                    batches_done = epoch * len(dataloaders[phase]) + batch_i\n",
        "                    batches_left = num_epochs * len(dataloaders[phase]) - batches_done\n",
        "\n",
        "                    sys.stdout.write(\n",
        "                        \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f), Acc: %.2f%% (%.2f%%)]\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            num_epochs,\n",
        "                            batch_i,\n",
        "                            len(dataloaders[phase]),\n",
        "                            loss.item(),\n",
        "                            np.mean(epoch_metrics[\"loss\"]),\n",
        "                            acc,\n",
        "                            np.mean(epoch_metrics[\"acc\"]),\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            # Epoch summary\n",
        "            epoch_acc = np.mean(epoch_metrics[\"acc\"])\n",
        "            epoch_loss = np.mean(epoch_metrics[\"loss\"])\n",
        "            print(f'\\n{phase.capitalize()} Epoch {epoch} - Acc: {epoch_acc:.2f}%, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "            # Save metrics for training phase\n",
        "            if phase == 'train':\n",
        "                acc_all.append(epoch_acc)\n",
        "                loss_all.append(epoch_loss)\n",
        "\n",
        "            # Save best model on validation\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_path = f'/content/drive/MyDrive/actionRecognition/weights_crime/c3d_best.h5'\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "            # Save regular checkpoint\n",
        "            if phase == 'train':\n",
        "                torch.save(model.state_dict(),\n",
        "                         f'/content/drive/MyDrive/actionRecognition/weights_crime/c3d_{epoch}.h5')\n",
        "\n",
        "        # Clear GPU cache\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\nPerforming final evaluation...\")\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    model.eval()\n",
        "\n",
        "    eval_metrics = {\"loss\": [], \"acc\": []}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloaders['val']:\n",
        "            image_sequences = X.to(device)\n",
        "            labels = y.to(device)\n",
        "\n",
        "            predictions = model(image_sequences)\n",
        "            loss = cls_criterion(predictions, labels)\n",
        "            acc = 100 * (predictions.argmax(1) == labels).cpu().numpy().mean()\n",
        "\n",
        "            eval_metrics[\"loss\"].append(loss.item())\n",
        "            eval_metrics[\"acc\"].append(acc)\n",
        "\n",
        "    final_acc = np.mean(eval_metrics[\"acc\"])\n",
        "    final_loss = np.mean(eval_metrics[\"loss\"])\n",
        "\n",
        "    print(f\"\\nFinal Evaluation Results:\")\n",
        "    print(f\"Best Validation Accuracy: {best_acc:.2f}%\")\n",
        "    print(f\"Final Test Accuracy: {final_acc:.2f}%\")\n",
        "    print(f\"Final Test Loss: {final_loss:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'training_acc': acc_all,\n",
        "        'training_loss': loss_all,\n",
        "        'best_acc': best_acc,\n",
        "        'final_acc': final_acc,\n",
        "        'best_model_path': best_model_path\n",
        "    }"
      ],
      "metadata": {
        "id": "kc02Q-gk_fOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "# torch.amp.GradScaler('cuda',\n",
        "results = train_and_evaluate(\n",
        "    model=model,\n",
        "    dataloaders=dataloaders,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=num_epochs,\n",
        "    device=device,\n",
        "    onecyc=onecyc\n",
        ")\n",
        "\n",
        "# Access results\n",
        "print(f\"Best validation accuracy: {results['best_acc']:.2f}%\")\n",
        "print(f\"Best model saved at: {results['best_model_path']}\")"
      ],
      "metadata": {
        "id": "koDLka8P_m2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoXZ08iHXbD2"
      },
      "source": [
        "loss_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtVNjFyeWVrd"
      },
      "source": [
        "error_plot(loss_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dybBWSyyWZfY"
      },
      "source": [
        "acc_plot(acc_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "def load_and_preprocess_video(video_path, sequence_length=16, im_size=128):\n",
        "    \"\"\"Load and preprocess video for inference\"\"\"\n",
        "    mean = [0.4889, 0.4887, 0.4891]\n",
        "    std = [0.2074, 0.2074, 0.2074]\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((im_size, im_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "    # Read video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convert BGR to RGB\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Handle videos shorter than sequence_length\n",
        "    if frame_count < sequence_length:\n",
        "        last_frame = frames[-1]\n",
        "        for _ in range(sequence_length - frame_count):\n",
        "            frames.append(last_frame)\n",
        "\n",
        "    # Sample sequence_length frames uniformly\n",
        "    indices = np.linspace(0, len(frames) - 1, sequence_length, dtype=int)\n",
        "    frames = [frames[i] for i in indices]\n",
        "\n",
        "    # Apply transforms\n",
        "    frames = [transform(Image.fromarray(frame)) for frame in frames]\n",
        "\n",
        "    # Stack frames\n",
        "    frames = torch.stack(frames)\n",
        "\n",
        "    return frames.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "def classify_video(model_path, video_path, device='cuda', class_labels=None):\n",
        "    \"\"\"\n",
        "    Load model and classify a video\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the .h5 model weights\n",
        "        video_path: Path to the video file\n",
        "        device: 'cuda' or 'cpu'\n",
        "        class_labels: List of class names (optional)\n",
        "    \"\"\"\n",
        "    # Load the model architecture (you need to define this based on your model)\n",
        "    # model = YourModelClass()  # Replace with your model architecture\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocess video\n",
        "    input_tensor = load_and_preprocess_video(video_path)\n",
        "    input_tensor = input_tensor.to(device)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        predictions = model(input_tensor)\n",
        "        probabilities = torch.nn.functional.softmax(predictions, dim=1)\n",
        "\n",
        "        # Get the predicted class\n",
        "        pred_class = torch.argmax(probabilities, dim=1).item()\n",
        "        confidence = probabilities[0][pred_class].item()\n",
        "\n",
        "    # Format results\n",
        "    result = {\n",
        "        'predicted_class_idx': pred_class,\n",
        "        'confidence': confidence * 100,  # Convert to percentage\n",
        "        'class_name': class_labels[pred_class] if class_labels else f\"Class {pred_class}\",\n",
        "        'probabilities': probabilities[0].cpu().numpy()\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Define your class labels\n",
        "    class_labels = ['Robbery', 'Vandalism', 'Shoplifting', 'normal', 'Burglary', 'Stealing']\n",
        "\n",
        "    # Configuration\n",
        "    model_path = '/content/drive/MyDrive/actionRecognition/weights_crime/c3d_best.h5'\n",
        "    video_path = '/content/drive/MyDrive/actionRecognition/anomaly_videos/Stealing/Stealing002_x264.mp4'\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Classify video\n",
        "    result = classify_video(\n",
        "        model_path=model_path,\n",
        "        video_path=video_path,\n",
        "        device=device,\n",
        "        class_labels=class_labels\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nPrediction Results:\")\n",
        "    print(f\"Predicted Class: {result['class_name']}\")\n",
        "    print(f\"Confidence: {result['confidence']:.2f}%\")\n",
        "\n",
        "    # Print all class probabilities\n",
        "    if class_labels:\n",
        "        print(\"\\nClass Probabilities:\")\n",
        "        for i, prob in enumerate(result['probabilities']):\n",
        "            print(f\"{class_labels[i]}: {prob*100:.2f}%\")"
      ],
      "metadata": {
        "id": "spQ-Tzy-AXLL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "c7cbc6d5-eb2b-44db-8f78-616ea740e961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'model' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-28b844224f17>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# Classify video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     result = classify_video(\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mvideo_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-28b844224f17>\u001b[0m in \u001b[0;36mclassify_video\u001b[0;34m(model_path, video_path, device, class_labels)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Load the model architecture (you need to define this based on your model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# model = YourModelClass()  # Replace with your model architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'model' where it is not associated with a value"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from model import resnet50\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from clr import OneCycle\n",
        "\n",
        "def load_and_preprocess_video(video_path, sequence_length=16, im_size=128):\n",
        "    \"\"\"Load and preprocess video for inference\"\"\"\n",
        "    mean = [0.4889, 0.4887, 0.4891]\n",
        "    std = [0.2074, 0.2074, 0.2074]\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((im_size, im_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "    # Read video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = Image.fromarray(frame)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Handle videos shorter than sequence_length\n",
        "    if frame_count < sequence_length:\n",
        "        last_frame = frames[-1]\n",
        "        for _ in range(sequence_length - frame_count):\n",
        "            frames.append(last_frame)\n",
        "\n",
        "    # Sample sequence_length frames uniformly\n",
        "    indices = np.linspace(0, len(frames) - 1, sequence_length, dtype=int)\n",
        "    frames = [frames[i] for i in indices]\n",
        "\n",
        "    # Apply transforms\n",
        "    transformed_frames = [transform(frame) for frame in frames]\n",
        "\n",
        "    # Stack frames along a new dimension (temporal)\n",
        "    # Change from [T, C, H, W] to [C, T, H, W]\n",
        "    frames_tensor = torch.stack(transformed_frames, dim=1)  # Changed from dim=0 to dim=1\n",
        "\n",
        "    return frames_tensor.unsqueeze(0)  # Add batch dimension [B, C, T, H, W]\n",
        "\n",
        "\n",
        "def classify_video(model_path, video_path, device='cuda', class_labels=None):\n",
        "    \"\"\"Load ResNet50 model and classify a video\"\"\"\n",
        "    # Initialize model\n",
        "    model = resnet50(class_num=len(class_labels)).to(device)  # Update to match number of classes\n",
        "    # model = resnet50(class_num=6).to(device)  # Update to match number of classes\n",
        "\n",
        "    # Load weights\n",
        "    # model.load_state_dict(torch.load(model_path))\n",
        "    #  map_location='cpu'\n",
        "\n",
        "    # model.load_state_dict(torch.load(model_path, weights_only=True))\n",
        "    model.load_state_dict(torch.load(model_path,     map_location=device, weights_only=True))\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocess video\n",
        "    input_tensor = load_and_preprocess_video(video_path)\n",
        "    input_tensor = input_tensor.to(device)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        predictions = model(input_tensor)\n",
        "        probabilities = torch.nn.functional.softmax(predictions, dim=1)\n",
        "\n",
        "        pred_class = torch.argmax(probabilities, dim=1).item()\n",
        "        confidence = probabilities[0][pred_class].item()\n",
        "\n",
        "    result = {\n",
        "        'predicted_class_idx': pred_class,\n",
        "        'confidence': confidence * 100,\n",
        "        'class_name': class_labels[pred_class] if class_labels else f\"Class {pred_class}\",\n",
        "        'probabilities': probabilities[0].cpu().numpy()\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Your actual classes\n",
        "    class_labels = ['Robbery', 'Vandalism', 'Shoplifting', 'normal', 'Burglary', 'Stealing']\n",
        "\n",
        "    # Configuration\n",
        "    model_path = '/content/drive/MyDrive/actionRecognition/weights_crime/c3d_best_v1.h5'\n",
        "    # video_path = '/content/drive/MyDrive/actionRecognition/anomaly_videos/Stealing/Stealing016_x264.mp4'\n",
        "    video_path = '/content/4fcf293d-7d6a-4da0-bcef-c9b5dd6404f6.mp4'\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Classify video\n",
        "    try:\n",
        "        result = classify_video(\n",
        "            model_path=model_path,\n",
        "            video_path=video_path,\n",
        "            device=device,\n",
        "            class_labels=class_labels\n",
        "        )\n",
        "\n",
        "        # Print results\n",
        "        print(f\"\\nPrediction Results:\")\n",
        "        print(f\"Predicted Class: {result['class_name']}\")\n",
        "        print(f\"Confidence: {result['confidence']:.2f}%\")\n",
        "\n",
        "        # Print all class probabilities\n",
        "        print(\"\\nClass Probabilities:\")\n",
        "        for i, prob in enumerate(result['probabilities']):\n",
        "            if i < len(class_labels):  # Make sure we don't exceed the number of labels\n",
        "                print(f\"{class_labels[i]}: {prob*100:.2f}%\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "EBlBIlzOFRR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcf69036-c099-435c-c03e-f27ecbbdae7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction Results:\n",
            "Predicted Class: Stealing\n",
            "Confidence: 100.00%\n",
            "\n",
            "Class Probabilities:\n",
            "Robbery: 0.00%\n",
            "Vandalism: 0.00%\n",
            "Shoplifting: 0.00%\n",
            "normal: 0.00%\n",
            "Burglary: 0.00%\n",
            "Stealing: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python pillow"
      ],
      "metadata": {
        "id": "XRuWbncYv1dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "IMs1IOx82GQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2cb7462-0f02-42a6-de28-fb457de9828b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Videoclassification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from model import resnet50\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import time\n",
        "from collections import deque\n",
        "import logging\n",
        "\n",
        "class RealTimeActionDetector:\n",
        "    def __init__(self, model_path, class_labels, device='cuda',\n",
        "                 sequence_length=16, im_size=128,\n",
        "                 confidence_threshold=50.0):\n",
        "        \"\"\"\n",
        "        Initialize the real-time action detection system\n",
        "\n",
        "        :param model_path: Path to the trained model weights\n",
        "        :param class_labels: List of class labels\n",
        "        :param device: Computing device (cuda/cpu)\n",
        "        :param sequence_length: Number of frames to process\n",
        "        :param im_size: Image resize dimension\n",
        "        :param confidence_threshold: Minimum confidence to trigger an alert\n",
        "        \"\"\"\n",
        "        # Model and preprocessing setup\n",
        "        self.device = device if torch.cuda.is_available() else 'cpu'\n",
        "        self.class_labels = class_labels\n",
        "        self.sequence_length = sequence_length\n",
        "        self.im_size = im_size\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = self._initialize_model(model_path)\n",
        "\n",
        "        # Transform setup\n",
        "        self.transform = self._create_transform()\n",
        "\n",
        "        # Logging setup\n",
        "        logging.basicConfig(level=logging.INFO,\n",
        "                            format='%(asctime)s - %(levelname)s: %(message)s')\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        # Frame buffer\n",
        "        self.frame_buffer = deque(maxlen=sequence_length)\n",
        "\n",
        "    def _initialize_model(self, model_path):\n",
        "        \"\"\"Initialize the model\"\"\"\n",
        "        model = resnet50(class_num=len(self.class_labels)).to(self.device)\n",
        "        model.load_state_dict(torch.load(model_path,\n",
        "                                         map_location=self.device,\n",
        "                                         weights_only=True))\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def _create_transform(self):\n",
        "        \"\"\"Create image transformation pipeline\"\"\"\n",
        "        mean = [0.4889, 0.4887, 0.4891]\n",
        "        std = [0.2074, 0.2074, 0.2074]\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((self.im_size, self.im_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "\n",
        "    def _process_frame_sequence(self, frames):\n",
        "        \"\"\"Process a sequence of frames\"\"\"\n",
        "        transformed_frames = [\n",
        "            self.transform(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))\n",
        "            for frame in frames\n",
        "        ]\n",
        "        frames_tensor = torch.stack(transformed_frames, dim=1)\n",
        "        return frames_tensor.unsqueeze(0).to(self.device)\n",
        "\n",
        "    def detect_action(self, frame_buffer):\n",
        "        \"\"\"\n",
        "        Detect action in the frame sequence\n",
        "\n",
        "        :param frame_buffer: List of frames\n",
        "        :return: Detection results\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            input_tensor = self._process_frame_sequence(list(frame_buffer))\n",
        "            predictions = self.model(input_tensor)\n",
        "            probabilities = torch.nn.functional.softmax(predictions, dim=1)\n",
        "\n",
        "            # Get top 2 predictions\n",
        "            top2_probs, top2_indices = torch.topk(probabilities[0], k=2)\n",
        "\n",
        "            results = {\n",
        "                'top_class': {\n",
        "                    'name': self.class_labels[top2_indices[0]],\n",
        "                    'probability': top2_indices[0].item() * 100\n",
        "                },\n",
        "                'second_class': {\n",
        "                    'name': self.class_labels[top2_indices[1]],\n",
        "                    'probability': top2_indices[1].item() * 100\n",
        "                }\n",
        "            }\n",
        "\n",
        "            return results\n",
        "\n",
        "    def process_camera_feed(self, camera_id=0):\n",
        "        \"\"\"\n",
        "        Real-time camera feed processing\n",
        "\n",
        "        :param camera_id: Camera index or stream URL\n",
        "        \"\"\"\n",
        "        cap = cv2.VideoCapture(camera_id)\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            self.logger.error(\"Could not open camera\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Add frame to buffer\n",
        "                self.frame_buffer.append(frame.copy())\n",
        "\n",
        "                # Process when buffer is full\n",
        "                if len(self.frame_buffer) == self.sequence_length:\n",
        "                    detection_results = self.detect_action(self.frame_buffer)\n",
        "\n",
        "                    # Check if detection meets confidence threshold\n",
        "                    if (detection_results['top_class']['probability'] >= self.confidence_threshold and\n",
        "                        detection_results['top_class']['name'] != 'normal'):\n",
        "\n",
        "                        # Log and potentially trigger alert\n",
        "                        alert_message = (\n",
        "                            f\"ALERT: Detected {detection_results['top_class']['name']} \"\n",
        "                            f\"(Prob: {detection_results['top_class']['probability']:.2f}%) \"\n",
        "                            f\"with {detection_results['second_class']['name']} \"\n",
        "                            f\"(Prob: {detection_results['second_class']['probability']:.2f}%)\"\n",
        "                        )\n",
        "\n",
        "                        self.logger.warning(alert_message)\n",
        "\n",
        "                        # Here you can add email notification function call\n",
        "                        # send_email(alert_message)\n",
        "\n",
        "                # Display frame with detection info\n",
        "                cv2.putText(frame,\n",
        "                            f\"Top: {detection_results['top_class']['name']} \"\n",
        "                            f\"({detection_results['top_class']['probability']:.2f}%)\",\n",
        "                            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "                cv2.putText(frame,\n",
        "                            f\"Second: {detection_results['second_class']['name']} \"\n",
        "                            f\"({detection_results['second_class']['probability']:.2f}%)\",\n",
        "                            (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "                cv2.imshow('Real-time Action Detection', frame)\n",
        "\n",
        "                # Break on 'q' press\n",
        "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                    break\n",
        "\n",
        "        finally:\n",
        "            cap.release()\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    class_labels = ['Robbery', 'Vandalism', 'Shoplifting', 'normal', 'Burglary', 'Stealing']\n",
        "    model_path = '/content/drive/MyDrive/actionRecognition/weights_crime/c3d_best_v1.h5'\n",
        "\n",
        "    # Initialize detector\n",
        "    detector = RealTimeActionDetector(\n",
        "        model_path=model_path,\n",
        "        class_labels=class_labels,\n",
        "        confidence_threshold=50.0  # Adjust as needed\n",
        "    )\n",
        "\n",
        "    # Start real-time processing\n",
        "    detector.process_camera_feed()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ZTkG0B_xt_oT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462a52c4-7216-4dad-ec4b-ac06ed48b0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Could not open camera\n"
          ]
        }
      ]
    }
  ]
}